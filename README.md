# Models
Because I forget.

## MNLI 
 - [Training](https://github.com/danyaljj/fairseq/blob/master/examples/roberta/README.glue.md) 
 - [Model files](https://drive.google.com/drive/folders/1ysmtlOJo7qGypLRqyJeHe_CoG6mgsAhl?usp=sharing)
 - [Inference and evaluation](https://github.com/danyaljj/fairseq/blob/master/examples/roberta/glue_inference.py).  
 - [Predictions](https://github.com/danyaljj/fairseq/tree/master/examples/roberta/glue_data/MNLI/predictions)
 - Evaluation output:  
 ```
 loading archive file /Users/danielk/ideaProjects/fairseq/examples/roberta/mnli-checkpoints/
loading archive file MNLI-bin
| [input] dictionary: 50265 types
| [label] dictionary: 9 types
201it [00:59,  3.69it/s]| Accuracy:  0.8910891089108911

loading archive file /Users/danielk/ideaProjects/fairseq/examples/roberta/mnli-checkpoints/
loading archive file MNLI-bin
| [input] dictionary: 50265 types
| [label] dictionary: 9 types
201it [01:05,  3.07it/s]
| Accuracy:  0.8663366336633663 . # the overall score was actually higher (close to 90%). This is only on the first 200 instances. 
 ```
 

## SNLI 

## BoolQ 


## SQuAD 1.1 

## SQuAD 2.0 

## SciTail 

## 
